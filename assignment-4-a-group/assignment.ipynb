{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe7e4593",
   "metadata": {},
   "source": [
    "# Assignment 4: Testing a Monte Carlo Simulation\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "In this assignment, you receive code that does a simple Monte Carlo simulation on the effect of measurement error in a linear regression. \n",
    "\n",
    "Many people say that code for Monte Carlo simulation is untestable. After all, we simulate because we do not know what will come out of the simulation. And to make it worse, there is randomness involved.\n",
    "\n",
    "In this assignment, your task is to prove them wrong by producing well-tested Monte Carlo code.\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "- In the screencast and other places you might have seen that people use `np.random.seed(seed)` to make randomness reproducible. This is ok for very simple cases, but in this assignment and the rest of the class we will use the more modern and explicit way of handling randomness via random number generators (via `rng = np.random.default_rng(seed)`. Background on this can be found in this [excellent blogpost](https://albertcthomas.github.io/good-practices-random-number-generators/). The starter code already uses this approach.\n",
    "- After some tasks we ask you to commit and set a tag in git. This is so we can easily check out the version of your repo after this step. This [video](https://www.youtube.com/watch?v=govmXpDGLpo) explains all you need to know about tags. The tags should always be set on the main branch. \n",
    "- **Please do the tasks in order**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a2404",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "1. Clone this repository, create the environment, activate it, and run `pre-commit install`. \n",
    "\n",
    "1. Look at the starter code in `run.py`. Explain to each other what the code does. Run the file and and look at the plot it produces. Discuss in the group what you like and dislike about the code in `run.py`. Write it down in bullet points in a markdown file called `answers.md`. \n",
    "\n",
    "1. A minimal requirement to make the Monte Carlo code testable is to put it into a function. Hence you will implement the `do_monte_carlo` function in `monte_carlo.py`. \n",
    "   - The inputs defined at the top of `run.py` will become function arguments. \n",
    "   - `data` will be returned by the function.\n",
    "   - Write a docstring for the function that describes all inputs and outputs. Use a [google style docstring](https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html), i.e. the formatting that you have seen in docstrings we previously gave you. \n",
    "   - Import the new function in `run.py` and call it with the inputs. \n",
    "   \n",
    "1. Set the tag `v1.0` and push it to GitHub.\n",
    "\n",
    "1. If only one x-variable is measured with classical measurement error, you get attenuation bias, i.e. the parameter of the variable with measurement error is biased towards zero. Use this fact to write a test for the `do_monte_carlo` function in the module `test_monte_carlo.py`.\n",
    "\n",
    "   You can use similar inputs as the ones used in `run.py` but maybe make them smaller (e.g. fewer parameters) to make the test faster.  \n",
    "\n",
    "1. Is this test enough? What are possible drawbacks? What is good about this test? Write down some bullet points in `answers.md`. Relate it to what you learned in the screencasts.\n",
    "\n",
    "1. Go over each line in `monte_carlo.py` and decide whether the output of that line is deterministic or random. Example: \n",
    "   - `epsilon = rng.normal(...)` is random.\n",
    "   - `y = x @ true_params + epsilon` is deterministic. This happens even though epsilon is random. The reason is that given a value of epsilon, all calculations are deterministic. \n",
    "\n",
    "1. Split the code into smaller subfunctions. The names of the subfunctions should start with an underscore to mark that they are not meant to be called directly by a user. The interface and behaviour of `do_monte_carlo` should not change. The test you already wrote should continue to pass. When splitting the code into functions, keep the following trade-offs in mind.\n",
    "\n",
    "   - It is easier to come up with test cases if functions are very very small. However, in the extreme case where each function is executing a single operation, your unit tests rely on the implementation and not just interfaces.\n",
    "   - Lines with randomness make it harder to test a function. For that reason, they should be separated from deterministic lines. However, code becomes more readable by grouping into a function all lines that comprise a logical step.\n",
    "   \n",
    "   Write docstrings for **all** new functions. Except for the docstrings, there should be no need for comments in the code after you are done. If you still think comments are necessary, it is a sign that your function names are not good. \n",
    "\n",
    "1. Write tests for **all** new functions. If you want, describe your reasoning in `answers.md`. For example, how do you test functions with randomness?\n",
    "\n",
    "1. Set the tag `v2.0` and push it to GitHub.\n",
    "\n",
    "1. Until now, we have mainly thought about the happy path where we have a perfect user who only provides valid inputs. Add some test cases with invalid inputs (e.g. a negative standard deviation for the measurement error or a parameter vector that contains strings. Those tests are expected to fail, for now, so mark them with `@pytest.mark.xfail`.\n",
    "\n",
    "1. For each xfailed test, think about which [type of Exception](https://docs.python.org/3.10/library/exceptions.html) you would like to get and what the message should tell you. Use `with pytest.raises()` to actually test that you get the right exception. The test will still fail because you have not implemented the error handling yet. \n",
    "\n",
    "1. Set the tag `v3.0` and push it to GitHub.\n",
    "\n",
    "1. Remove the `@pytest.mark.xfail` decorators. Run the test and let it fail once, just to get the experience of test driven development.\n",
    "\n",
    "   Now implement the error handling and make the test pass.\n",
    "   \n",
    "   **Note**: All the error handling should be done in the one public function that will be called by the user. If you handle errors there in an exhaustive fashion, there is no need for any kind of `assert` statements or error handling inside the private functions. \n",
    "   \n",
    "1. Set the tag `v4.0` and push it to GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218173c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assignment_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:16:33) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd885df64be31788f2722cf6a66fc1198b10756bdef8eed583efa071321d6aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
